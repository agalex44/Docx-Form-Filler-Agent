{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsRdCbhVCW4-",
        "outputId": "40737c85-2827-47d3-dfc6-39f2e246218b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m165.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q python-docx google-generativeai rapidfuzz openpyxl pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "import hashlib\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "import io\n",
        "\n",
        "import google.generativeai as genai\n",
        "from docx import Document\n",
        "from docx.shared import RGBColor, Pt\n",
        "from docx.oxml import OxmlElement\n",
        "from rapidfuzz import fuzz\n",
        "import csv\n",
        "import openpyxl\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "8_UcSxrdCeL5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Gemini API\n",
        "GEMINI_API_KEY = \"GEMINI_API_KEY\"  # REPLACE THIS\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "print(\"âœ“ All dependencies loaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih7UsjcQChh8",
        "outputId": "a0e1a55d-d522-47cc-99df-774c946e0f54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All dependencies loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: INPUT NORMALIZER\n",
        "# ============================================================================\n",
        "class InputNormalizer:\n",
        "    \"\"\"Normalize and load input data from various formats\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_text(text: Any) -> str:\n",
        "        \"\"\"NFC Unicode normalization\"\"\"\n",
        "        if text is None:\n",
        "            return \"\"\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        return unicodedata.normalize('NFC', text.strip())\n",
        "\n",
        "    @staticmethod\n",
        "    def load_json(file_path: str) -> Dict[str, str]:\n",
        "        \"\"\"Load and normalize JSON\"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        normalized = {}\n",
        "        for k, v in data.items():\n",
        "            key = InputNormalizer.normalize_text(k)\n",
        "            value = InputNormalizer.normalize_text(v)\n",
        "            normalized[key] = value\n",
        "\n",
        "        return normalized\n",
        "\n",
        "    @staticmethod\n",
        "    def load_csv(file_path: str) -> Dict[str, str]:\n",
        "        \"\"\"Load CSV (key-value pairs)\"\"\"\n",
        "        normalized = {}\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            next(reader, None)  # Skip header if exists\n",
        "            for row in reader:\n",
        "                if len(row) >= 2:\n",
        "                    key = InputNormalizer.normalize_text(row[0])\n",
        "                    value = InputNormalizer.normalize_text(row[1])\n",
        "                    normalized[key] = value\n",
        "        return normalized\n",
        "\n",
        "    @staticmethod\n",
        "    def load_xlsx(file_path: str) -> Dict[str, str]:\n",
        "        \"\"\"Load XLSX (first sheet, first two columns)\"\"\"\n",
        "        df = pd.read_excel(file_path, sheet_name=0)\n",
        "        normalized = {}\n",
        "        for _, row in df.iterrows():\n",
        "            if len(row) >= 2:\n",
        "                key = InputNormalizer.normalize_text(row.iloc[0])\n",
        "                value = InputNormalizer.normalize_text(row.iloc[1])\n",
        "                if key:\n",
        "                    normalized[key] = value\n",
        "        return normalized\n",
        "\n",
        "    @staticmethod\n",
        "    def auto_load(file_path: str) -> Dict[str, str]:\n",
        "        \"\"\"Auto-detect format and load\"\"\"\n",
        "        ext = Path(file_path).suffix.lower()\n",
        "        if ext == '.json':\n",
        "            return InputNormalizer.load_json(file_path)\n",
        "        elif ext == '.csv':\n",
        "            return InputNormalizer.load_csv(file_path)\n",
        "        elif ext in ['.xlsx', '.xls']:\n",
        "            return InputNormalizer.load_xlsx(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {ext}\")\n"
      ],
      "metadata": {
        "id": "KoSasXH0C2jr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: PLACEHOLDER DETECTOR\n",
        "# ============================================================================\n",
        "class PlaceholderDetector:\n",
        "    \"\"\"Detect visual placeholders in DOCX\"\"\"\n",
        "\n",
        "    # Placeholder patterns\n",
        "    PATTERNS = {\n",
        "        'underscores': re.compile(r'_{5,}'),\n",
        "        'dots': re.compile(r'\\.{4,}'),\n",
        "        'brackets': re.compile(r'\\{[^}]+\\}'),\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_from_paragraphs(paragraphs) -> List[Tuple[str, str, Any]]:\n",
        "        \"\"\"Extract placeholders from paragraph list\"\"\"\n",
        "        placeholders = []\n",
        "        for para in paragraphs:\n",
        "            text = para.text\n",
        "            for pattern in PlaceholderDetector.PATTERNS.values():\n",
        "                for match in pattern.finditer(text):\n",
        "                    # Get surrounding context (100 chars)\n",
        "                    start = max(0, match.start() - 100)\n",
        "                    end = min(len(text), match.end() + 100)\n",
        "                    context = text[start:end].strip()\n",
        "                    placeholders.append((match.group(), context, para))\n",
        "        return placeholders\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_all(doc: Document) -> List[Tuple[str, str, Any]]:\n",
        "        \"\"\"Extract all placeholders from document\"\"\"\n",
        "        placeholders = []\n",
        "\n",
        "        # Body paragraphs\n",
        "        placeholders.extend(PlaceholderDetector.extract_from_paragraphs(doc.paragraphs))\n",
        "\n",
        "        # Tables\n",
        "        for table in doc.tables:\n",
        "            for row in table.rows:\n",
        "                for cell in row.cells:\n",
        "                    placeholders.extend(\n",
        "                        PlaceholderDetector.extract_from_paragraphs(cell.paragraphs)\n",
        "                    )\n",
        "\n",
        "        # Headers and footers\n",
        "        for section in doc.sections:\n",
        "            if section.header:\n",
        "                placeholders.extend(\n",
        "                    PlaceholderDetector.extract_from_paragraphs(section.header.paragraphs)\n",
        "                )\n",
        "            if section.footer:\n",
        "                placeholders.extend(\n",
        "                    PlaceholderDetector.extract_from_paragraphs(section.footer.paragraphs)\n",
        "                )\n",
        "\n",
        "        return placeholders"
      ],
      "metadata": {
        "id": "luIxUJ_TC8LG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: GEMINI FIELD MAPPER\n",
        "# ============================================================================\n",
        "class GeminiMapper:\n",
        "    \"\"\"Use Gemini to map JSON fields to placeholders\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"gemini-3.0-pro-preview\"):\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.generation_config = {\n",
        "            'temperature': 0.0,\n",
        "            'top_p': 0.95,\n",
        "            'top_k': 40,\n",
        "            'max_output_tokens': 8192,\n",
        "        }\n",
        "\n",
        "    def create_mapping_prompt(\n",
        "        self,\n",
        "        json_keys: List[str],\n",
        "        placeholders: List[Tuple[str, str, Any]]\n",
        "    ) -> str:\n",
        "        \"\"\"Create deterministic mapping prompt\"\"\"\n",
        "\n",
        "        # Limit to first 100 placeholders for prompt size\n",
        "        sample_placeholders = [\n",
        "            {\"placeholder\": ph, \"context\": ctx}\n",
        "            for ph, ctx, _ in placeholders[:100]\n",
        "        ]\n",
        "\n",
        "        prompt = f\"\"\"You are a Romanian document form filler. Map JSON field names to document placeholders.\n",
        "\n",
        "JSON FIELDS AVAILABLE:\n",
        "{json.dumps(json_keys, ensure_ascii=False, indent=2)}\n",
        "\n",
        "PLACEHOLDERS IN DOCUMENT (with context):\n",
        "{json.dumps(sample_placeholders, ensure_ascii=False, indent=2)}\n",
        "\n",
        "MAPPING RULES:\n",
        "1. Match based on semantic meaning (e.g., \"OPERATOR ECONOMIC\" matches \"Denumirea/numele operatorului economic\")\n",
        "2. Romanian diacritics: Äƒâ‰ˆa, Ã®â‰ˆi, È™â‰ˆs, È›â‰ˆt for fuzzy matching\n",
        "3. Partial matches allowed if semantically correct\n",
        "4. If multiple JSON keys match, choose most specific\n",
        "5. Only map if confidence is high\n",
        "6. Return ONLY valid JSON, no markdown, no explanation\n",
        "\n",
        "OUTPUT FORMAT (strict JSON):\n",
        "{{\n",
        "  \"placeholder1\": \"JSON_FIELD_NAME_1\",\n",
        "  \"placeholder2\": \"JSON_FIELD_NAME_2\"\n",
        "}}\n",
        "\n",
        "Generate mapping now:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def map_fields(\n",
        "        self,\n",
        "        json_keys: List[str],\n",
        "        placeholders: List[Tuple[str, str, Any]],\n",
        "        max_retries: int = 2\n",
        "    ) -> Dict[str, str]:\n",
        "        \"\"\"Map fields using Gemini with retry logic\"\"\"\n",
        "\n",
        "        prompt = self.create_mapping_prompt(json_keys, placeholders)\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = self.model.generate_content(\n",
        "                    prompt,\n",
        "                    generation_config=self.generation_config\n",
        "                )\n",
        "\n",
        "                # Extract JSON from response\n",
        "                text = response.text.strip()\n",
        "                # Remove markdown fences if present\n",
        "                text = re.sub(r'^```json\\s*|\\s*```$', '', text, flags=re.MULTILINE | re.DOTALL)\n",
        "                text = text.strip()\n",
        "\n",
        "                # Parse JSON\n",
        "                mapping = json.loads(text)\n",
        "\n",
        "                # Validate mapping\n",
        "                valid_mapping = {}\n",
        "                for placeholder, field_name in mapping.items():\n",
        "                    if field_name in json_keys:\n",
        "                        valid_mapping[placeholder] = field_name\n",
        "\n",
        "                print(f\"  Gemini mapped {len(valid_mapping)} fields\")\n",
        "                return valid_mapping\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    print(\"  Falling back to fuzzy matching\")\n",
        "                    return {}\n",
        "\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "X3G7TqmlDBIH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: FUZZY MATCHER (FALLBACK)\n",
        "# ============================================================================\n",
        "class FuzzyMatcher:\n",
        "    \"\"\"Deterministic fuzzy matching fallback\"\"\"\n",
        "\n",
        "    ROMANIAN_DIACRITICS = {\n",
        "        'Äƒ': 'a', 'Ã¢': 'a', 'Ã®': 'i', 'È™': 's', 'È›': 't',\n",
        "        'Ä‚': 'A', 'Ã‚': 'A', 'ÃŽ': 'I', 'È˜': 'S', 'Èš': 'T'\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_romanian(text: str) -> str:\n",
        "        \"\"\"Normalize Romanian diacritics for matching\"\"\"\n",
        "        text = text.lower()\n",
        "        for old, new in FuzzyMatcher.ROMANIAN_DIACRITICS.items():\n",
        "            text = text.replace(old, new.lower())\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def match(\n",
        "        placeholder_context: str,\n",
        "        json_keys: List[str],\n",
        "        threshold: int = 80\n",
        "    ) -> Optional[str]:\n",
        "        \"\"\"Find best fuzzy match\"\"\"\n",
        "\n",
        "        norm_context = FuzzyMatcher.normalize_romanian(placeholder_context)\n",
        "\n",
        "        best_match = None\n",
        "        best_score = 0\n",
        "\n",
        "        for key in json_keys:\n",
        "            norm_key = FuzzyMatcher.normalize_romanian(key)\n",
        "\n",
        "            # Try multiple matching strategies\n",
        "            scores = [\n",
        "                fuzz.partial_ratio(norm_context, norm_key),\n",
        "                fuzz.token_sort_ratio(norm_context, norm_key),\n",
        "                fuzz.token_set_ratio(norm_context, norm_key)\n",
        "            ]\n",
        "\n",
        "            score = max(scores)\n",
        "\n",
        "            if score > best_score and score >= threshold:\n",
        "                best_score = score\n",
        "                best_match = key\n",
        "\n",
        "        return best_match"
      ],
      "metadata": {
        "id": "amPnOtclDF72"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: DOCX RENDERER\n",
        "# ============================================================================\n",
        "class DocxRenderer:\n",
        "    \"\"\"Fill placeholders preserving formatting\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def replace_in_paragraph(para, placeholder: str, value: str) -> bool:\n",
        "        \"\"\"Replace placeholder in paragraph preserving runs\"\"\"\n",
        "        full_text = para.text\n",
        "\n",
        "        if placeholder not in full_text:\n",
        "            return False\n",
        "\n",
        "        # Handle multiline values\n",
        "        if '\\n' in value:\n",
        "            value = value.replace('\\n', ' ')  # Collapse to single line\n",
        "\n",
        "        # Find which runs contain the placeholder\n",
        "        placeholder_start = full_text.find(placeholder)\n",
        "        placeholder_end = placeholder_start + len(placeholder)\n",
        "\n",
        "        current_pos = 0\n",
        "        run_ranges = []\n",
        "\n",
        "        for i, run in enumerate(para.runs):\n",
        "            run_start = current_pos\n",
        "            run_end = current_pos + len(run.text)\n",
        "            run_ranges.append((i, run_start, run_end, run))\n",
        "            current_pos = run_end\n",
        "\n",
        "        # Find overlapping runs\n",
        "        affected_runs = []\n",
        "        for i, start, end, run in run_ranges:\n",
        "            if start < placeholder_end and end > placeholder_start:\n",
        "                affected_runs.append((i, start, end, run))\n",
        "\n",
        "        if not affected_runs:\n",
        "            return False\n",
        "\n",
        "        # Simple case: placeholder in single run\n",
        "        if len(affected_runs) == 1:\n",
        "            i, start, end, run = affected_runs[0]\n",
        "            rel_start = placeholder_start - start\n",
        "            rel_end = rel_start + len(placeholder)\n",
        "            run.text = run.text[:rel_start] + value + run.text[rel_end:]\n",
        "            return True\n",
        "\n",
        "        # Complex case: placeholder spans multiple runs\n",
        "        # Rebuild affected runs\n",
        "        first_run_idx, first_start, first_end, first_run = affected_runs[0]\n",
        "        last_run_idx, last_start, last_end, last_run = affected_runs[-1]\n",
        "\n",
        "        # Build replacement text\n",
        "        before = full_text[:placeholder_start]\n",
        "        after = full_text[placeholder_end:]\n",
        "        new_text = before + value + after\n",
        "\n",
        "        # Clear all runs and set first run to new text\n",
        "        first_run.text = new_text\n",
        "        for i in range(first_run_idx + 1, last_run_idx + 1):\n",
        "            if i < len(para.runs):\n",
        "                para.runs[i].text = \"\"\n",
        "\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_all_occurrences(doc: Document, placeholder: str, value: str) -> int:\n",
        "        \"\"\"Fill all occurrences of placeholder in document\"\"\"\n",
        "        count = 0\n",
        "\n",
        "        # Body paragraphs\n",
        "        for para in doc.paragraphs:\n",
        "            if DocxRenderer.replace_in_paragraph(para, placeholder, value):\n",
        "                count += 1\n",
        "\n",
        "        # Tables\n",
        "        for table in doc.tables:\n",
        "            for row in table.rows:\n",
        "                for cell in row.cells:\n",
        "                    for para in cell.paragraphs:\n",
        "                        if DocxRenderer.replace_in_paragraph(para, placeholder, value):\n",
        "                            count += 1\n",
        "\n",
        "        # Headers and footers\n",
        "        for section in doc.sections:\n",
        "            if section.header:\n",
        "                for para in section.header.paragraphs:\n",
        "                    if DocxRenderer.replace_in_paragraph(para, placeholder, value):\n",
        "                        count += 1\n",
        "            if section.footer:\n",
        "                for para in section.footer.paragraphs:\n",
        "                    if DocxRenderer.replace_in_paragraph(para, placeholder, value):\n",
        "                        count += 1\n",
        "\n",
        "        return count\n"
      ],
      "metadata": {
        "id": "-kIvTMKoDJY9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: ARRAY HANDLER\n",
        "# ============================================================================\n",
        "class ArrayHandler:\n",
        "    \"\"\"Handle JSON arrays and structured data\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def is_json_array(value: str) -> bool:\n",
        "        \"\"\"Check if value is JSON array\"\"\"\n",
        "        try:\n",
        "            parsed = json.loads(value)\n",
        "            return isinstance(parsed, list)\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def format_inline(value: str) -> str:\n",
        "        \"\"\"Format array for inline display\"\"\"\n",
        "        try:\n",
        "            arr = json.loads(value)\n",
        "            if isinstance(arr, list):\n",
        "                if not arr:\n",
        "                    return \"\"\n",
        "\n",
        "                # List of strings\n",
        "                if all(isinstance(x, str) for x in arr):\n",
        "                    return \", \".join(arr)\n",
        "\n",
        "                # List of dicts (summarize)\n",
        "                if all(isinstance(x, dict) for x in arr):\n",
        "                    return f\"[{len(arr)} Ã®nregistrÄƒri]\"\n",
        "\n",
        "                # Mixed types\n",
        "                return str(arr)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return value\n"
      ],
      "metadata": {
        "id": "H7uRej90DNaW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: MAIN AGENT\n",
        "# ============================================================================\n",
        "class DocxFormFillerAgent:\n",
        "    \"\"\"Main orchestrator for form filling\"\"\"\n",
        "\n",
        "    def __init__(self, use_gemini: bool = True):\n",
        "        self.normalizer = InputNormalizer()\n",
        "        self.detector = PlaceholderDetector()\n",
        "        self.gemini_mapper = GeminiMapper() if use_gemini else None\n",
        "        self.fuzzy_matcher = FuzzyMatcher()\n",
        "        self.renderer = DocxRenderer()\n",
        "        self.array_handler = ArrayHandler()\n",
        "\n",
        "    def process(\n",
        "        self,\n",
        "        docx_path: str,\n",
        "        data_path: str,\n",
        "        output_path: str,\n",
        "        verbose: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Main processing pipeline\"\"\"\n",
        "\n",
        "        stats = {\n",
        "            'total_placeholders': 0,\n",
        "            'filled_placeholders': 0,\n",
        "            'gemini_mappings': 0,\n",
        "            'fuzzy_mappings': 0,\n",
        "            'unmapped': 0\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            print(\"=\" * 60)\n",
        "            print(\"DOCX FORM FILLER AGENT\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "        # Step 1: Load data\n",
        "        if verbose:\n",
        "            print(\"\\n[1/6] Loading input data...\")\n",
        "        data = self.normalizer.auto_load(data_path)\n",
        "        if verbose:\n",
        "            print(f\"  âœ“ Loaded {len(data)} fields\")\n",
        "\n",
        "        # Step 2: Load document\n",
        "        if verbose:\n",
        "            print(\"\\n[2/6] Loading DOCX template...\")\n",
        "        doc = Document(docx_path)\n",
        "\n",
        "        # Step 3: Detect placeholders\n",
        "        if verbose:\n",
        "            print(\"\\n[3/6] Detecting placeholders...\")\n",
        "        placeholders = self.detector.extract_all(doc)\n",
        "        stats['total_placeholders'] = len(placeholders)\n",
        "        if verbose:\n",
        "            print(f\"  âœ“ Found {len(placeholders)} placeholders\")\n",
        "\n",
        "        # Group by unique placeholder text\n",
        "        placeholder_groups = {}\n",
        "        for ph, ctx, obj in placeholders:\n",
        "            if ph not in placeholder_groups:\n",
        "                placeholder_groups[ph] = []\n",
        "            placeholder_groups[ph].append((ctx, obj))\n",
        "\n",
        "        # Step 4: Map fields\n",
        "        if verbose:\n",
        "            print(\"\\n[4/6] Mapping fields to placeholders...\")\n",
        "\n",
        "        # Try Gemini first\n",
        "        gemini_mapping = {}\n",
        "        if self.gemini_mapper:\n",
        "            try:\n",
        "                gemini_mapping = self.gemini_mapper.map_fields(\n",
        "                    list(data.keys()),\n",
        "                    placeholders\n",
        "                )\n",
        "                stats['gemini_mappings'] = len(gemini_mapping)\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"  âš  Gemini error: {e}\")\n",
        "\n",
        "        # Step 5: Fill document\n",
        "        if verbose:\n",
        "            print(\"\\n[5/6] Filling placeholders...\")\n",
        "\n",
        "        for placeholder_text, occurrences in placeholder_groups.items():\n",
        "            context, obj = occurrences[0]  # Use first occurrence for matching\n",
        "\n",
        "            # Try to find mapping\n",
        "            matched_key = None\n",
        "\n",
        "            # Try Gemini mapping\n",
        "            for mapped_ph, field in gemini_mapping.items():\n",
        "                if mapped_ph in placeholder_text or placeholder_text in mapped_ph:\n",
        "                    matched_key = field\n",
        "                    break\n",
        "\n",
        "            # Fallback to fuzzy matching\n",
        "            if not matched_key:\n",
        "                matched_key = self.fuzzy_matcher.match(context, list(data.keys()))\n",
        "                if matched_key:\n",
        "                    stats['fuzzy_mappings'] += 1\n",
        "\n",
        "            # Fill if we found a match\n",
        "            if matched_key and matched_key in data:\n",
        "                value = data[matched_key]\n",
        "\n",
        "                # Handle arrays\n",
        "                if self.array_handler.is_json_array(value):\n",
        "                    value = self.array_handler.format_inline(value)\n",
        "\n",
        "                # Fill all occurrences\n",
        "                count = self.renderer.fill_all_occurrences(doc, placeholder_text, value)\n",
        "                stats['filled_placeholders'] += count\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"  âœ“ [{count}x] {placeholder_text[:30]}... â†’ {matched_key[:40]}\")\n",
        "            else:\n",
        "                stats['unmapped'] += 1\n",
        "                if verbose:\n",
        "                    print(f\"  âœ— No match: ...{context[max(0, len(context)-50):]}\")\n",
        "\n",
        "        # Step 6: Save\n",
        "        if verbose:\n",
        "            print(\"\\n[6/6] Saving output...\")\n",
        "        doc.save(output_path)\n",
        "        if verbose:\n",
        "            print(f\"  âœ“ Saved: {output_path}\")\n",
        "\n",
        "        # Summary\n",
        "        if verbose:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"SUMMARY\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Total placeholders: {stats['total_placeholders']}\")\n",
        "            print(f\"Filled: {stats['filled_placeholders']}\")\n",
        "            print(f\"  - Via Gemini: {stats['gemini_mappings']}\")\n",
        "            print(f\"  - Via fuzzy: {stats['fuzzy_mappings']}\")\n",
        "            print(f\"Unmapped: {stats['unmapped']}\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "        return stats\n"
      ],
      "metadata": {
        "id": "s4f37pIkDYDO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10: VALIDATION TOOLS\n",
        "# ============================================================================\n",
        "class ValidationTools:\n",
        "    \"\"\"Tools for validating output quality\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def check_determinism(agent, docx_path: str, data_path: str, runs: int = 3) -> bool:\n",
        "        \"\"\"Verify deterministic output\"\"\"\n",
        "        print(\"\\n[DETERMINISM CHECK]\")\n",
        "        hashes = []\n",
        "\n",
        "        for i in range(runs):\n",
        "            output = f\"_temp_run_{i}.docx\"\n",
        "            agent.process(docx_path, data_path, output, verbose=False)\n",
        "\n",
        "            with open(output, 'rb') as f:\n",
        "                file_hash = hashlib.sha256(f.read()).hexdigest()\n",
        "            hashes.append(file_hash)\n",
        "            print(f\"  Run {i+1}: {file_hash[:16]}...\")\n",
        "\n",
        "            # Cleanup\n",
        "            os.remove(output)\n",
        "\n",
        "        unique_hashes = len(set(hashes))\n",
        "        if unique_hashes == 1:\n",
        "            print(\"  âœ“ PASS: All outputs identical\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"  âœ— FAIL: {unique_hashes} different outputs\")\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def check_unicode(filled_docx: str) -> bool:\n",
        "        \"\"\"Check Romanian diacritics preserved\"\"\"\n",
        "        print(\"\\n[UNICODE CHECK]\")\n",
        "        doc = Document(filled_docx)\n",
        "\n",
        "        all_text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "\n",
        "        diacritics = {'Äƒ', 'Ã¢', 'Ã®', 'È™', 'È›', 'Ä‚', 'Ã‚', 'ÃŽ', 'È˜', 'Èš'}\n",
        "        found = set(c for c in all_text if c in diacritics)\n",
        "\n",
        "        if found:\n",
        "            print(f\"  âœ“ PASS: Found diacritics: {found}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"  âš  WARNING: No diacritics found (may be OK if input has none)\")\n",
        "            return True\n"
      ],
      "metadata": {
        "id": "AdwyDNiSDZOd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize agent\n",
        "agent = DocxFormFillerAgent(use_gemini=True)\n",
        "\n",
        "# Process document\n",
        "stats = agent.process(\n",
        "    docx_path=\"sample_forms.docx\",\n",
        "    data_path=\"input_date.json\",\n",
        "    output_path=\"sample_forms.filled.docx\"\n",
        ")\n",
        "\n",
        "# Validation (optional)\n",
        "print(\"\\n\\nRUNNING VALIDATION...\")\n",
        "ValidationTools.check_determinism(agent, \"sample_forms.docx\", \"input_date.json\", runs=3)\n",
        "ValidationTools.check_unicode(\"sample_forms.filled.docx\")\n",
        "\n",
        "# Download result (Colab)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(\"sample_forms.filled.docx\")\n",
        "    print(\"\\nâœ“ Download started!\")\n",
        "except:\n",
        "    print(\"\\nâœ“ File saved locally (not in Colab environment)\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ COMPLETE!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hDxekQa4DdFM",
        "outputId": "5cbe818f-fb2d-43e6-b4e4-082bde43af57"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DOCX FORM FILLER AGENT\n",
            "============================================================\n",
            "\n",
            "[1/6] Loading input data...\n",
            "  âœ“ Loaded 83 fields\n",
            "\n",
            "[2/6] Loading DOCX template...\n",
            "\n",
            "[3/6] Detecting placeholders...\n",
            "  âœ“ Found 179 placeholders\n",
            "\n",
            "[4/6] Mapping fields to placeholders...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 580.24ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempt 1 failed: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 328.82ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempt 2 failed: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.\n",
            "  Falling back to fuzzy matching\n",
            "\n",
            "[5/6] Filling placeholders...\n",
            "  âœ— No match: ..._____________________\n",
            "  âœ— No match: .....................................................\n",
            "  âœ“ [4x] ______________________________... â†’ Denumirea / numele ofertantului\n",
            "  âœ— No match: ..._________________________________________________,\n",
            "  âœ— No match: ..._____________________________________________ lei.\n",
            "  âœ— No match: ...Äƒ ofertÄƒ valabilÄƒ pentru o duratÄƒ de _____________\n",
            "  âœ“ [60x] ___________________... â†’ Data\n",
            "  âœ“ [5x] _____________________________... â†’ Data\n",
            "  âœ“ [55x] _____... â†’ Data\n",
            "  âœ— No match: ... ______________________, legal autorizat sÄƒ semnez\n",
            "  âœ— No match: ... ______________________, legal autorizat sÄƒ semnez\n",
            "  âœ— No match: ...ÅŸi Ã®n numele ____________________________________.\n",
            "  âœ— No match: ...................................\n",
            "  âœ“ [5x] ................................. â†’ Subsemnatul - nume reprezentant imputern\n",
            "  âœ— No match: ...* sau Ã®n Å£ara Ã®n care este stabilit ..............\n",
            "  âœ— No match: ...OPERATOR ECONOMIC \t\t\t\t\t\t\t\t  ____________________\n",
            "  âœ“ [1x] ______________________________... â†’ Subsemnatul - nume reprezentant imputern\n",
            "  âœ“ [1x] ______________________________... â†’ Subsemnatul - nume reprezentant imputern\n",
            "  âœ“ [0x] ______________________________... â†’ Adresa ofertant\n",
            "  âœ— No match: ..._____________, declar pe proprie rÄƒspundere, urmÄƒt\n",
            "  âœ— No match: ...cÃ¢nd prevederile art. 59 È™i 60 din Legea nr. 98/20\n",
            "  âœ— No match: ...................., dna............................\n",
            "  âœ— No match: ..._________________\n",
            "  âœ— No match: .............................................\n",
            "  âœ— No match: .....................................................\n",
            "  âœ“ [8x] ................................. â†’ Denumirea contractului\n",
            "  âœ“ [34x] ................ â†’ Denumirea contractului\n",
            "  âœ“ [28x] ............. â†’ Denumirea tertului sustinator tehnic si \n",
            "  âœ“ [0x] ................................. â†’ Denumirea contractului\n",
            "  âœ“ [0x] ................................. â†’ Denumirea contractului de achizitie publ\n",
            "  âœ“ [9x] .......................... â†’ Denumirea tertului sustinator tehnic si \n",
            "  âœ“ [5x] ................... â†’ Denumirea contractului de achizitie publ\n",
            "  âœ“ [2x] ..................... â†’ Denumirea tertului sustinator tehnic si \n",
            "  âœ“ [0x] .................... â†’ Contractant\n",
            "  âœ“ [0x] ................................. â†’ Denumirea tertului sustinator tehnic si \n",
            "  âœ“ [0x] ................................. â†’ Data\n",
            "  âœ“ [0x] ______________________________... â†’ Numele in clar al persoanei autorizate\n",
            "  âœ— No match: ...i corecte Ã®n fiecare detaliu ÅŸi Ã®nÅ£eleg cÄƒ autorit\n",
            "  âœ“ [1x] __________________________... â†’ Contractant\n",
            "  âœ“ [0x] ......................... â†’ Data\n",
            "  âœ“ [0x] ................................. â†’ Subsemnatul - nume reprezentant imputern\n",
            "  âœ“ [0x] ................................. â†’ Contractant\n",
            "  âœ“ [40x] ______... â†’ Achizitor\n",
            "  âœ“ [14x] __________... â†’ Banca / societate de asigurare - denumir\n",
            "  âœ“ [5x] ________... â†’ Banca / societate de asigurare - denumir\n",
            "  âœ— No match: ......., Ã®n calitate de * ...........................\n",
            "  âœ— No match: ......., Ã®n calitate de * ...........................\n",
            "  âœ— No match: ........., Ã®n calitate de *..........................\n",
            "  âœ— No match: ........., Ã®n calitate de *..........................\n",
            "  âœ“ [0x] ................................. â†’ Autoritate contractanta\n",
            "  âœ“ [0x] ................................. â†’ Autoritate contractanta\n",
            "  âœ— No match: ...1. ___________________________________\n",
            "  âœ— No match: ...1. _______ % S.C. ___________________________\n",
            "  âœ— No match: ...1. _______ % S.C. ___________________________\n",
            "  âœ— No match: ........................, prevÄƒzute la art. .........\n",
            "  âœ— No match: ........................, prevÄƒzute la art. .........\n",
            "  âœ“ [32x] ........ â†’ Data\n",
            "  âœ— No match: .....................................................\n",
            "  âœ— No match: ...____ Ã®ncheiat Ã®ntre ______________________________\n",
            "  âœ— No match: ...__________ cu sediul Ã®n __________________________\n",
            "  âœ“ [2x] ___________... â†’ Contractant\n",
            "  âœ— No match: ...________________________________ (adresa,tel.,fax)\n",
            "  âœ— No match: ...de cÄƒtre beneficiarul __________________________ .\n",
            "  âœ“ [0x] ______________________________... â†’ Contractant\n",
            "  âœ“ [0x] ______________________________... â†’ Autoritate contractanta\n",
            "  âœ“ [0x] ................................. â†’ Cod CPV\n",
            "  âœ“ [0x] ................................. â†’ Autoritate contractanta\n",
            "  âœ“ [26x] ....... â†’ Atribut fiscal\n",
            "  âœ“ [0x] ................................ â†’ Data\n",
            "  âœ“ [0x] ............................ â†’ Data\n",
            "  âœ“ [0x] ...................... â†’ Ora\n",
            "  âœ— No match: .....................................................\n",
            "  âœ— No match: ... prin  ...........................................\n",
            "  âœ— No match: .................................................\n",
            "  âœ“ [0x] ______________________________... â†’ Denumirea / numele operatorului economic\n",
            "  âœ“ [0x] _________... â†’ Contractant\n",
            "\n",
            "[6/6] Saving output...\n",
            "  âœ“ Saved: sample_forms.filled.docx\n",
            "\n",
            "============================================================\n",
            "SUMMARY\n",
            "============================================================\n",
            "Total placeholders: 179\n",
            "Filled: 337\n",
            "  - Via Gemini: 0\n",
            "  - Via fuzzy: 41\n",
            "Unmapped: 35\n",
            "============================================================\n",
            "\n",
            "\n",
            "RUNNING VALIDATION...\n",
            "\n",
            "[DETERMINISM CHECK]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 429.90ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempt 1 failed: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 356.95ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempt 2 failed: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.\n",
            "  Falling back to fuzzy matching\n",
            "  Run 1: b0dfd5f8eedce9d1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 328.32ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempt 1 failed: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 328.21ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempt 2 failed: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.\n",
            "  Falling back to fuzzy matching\n",
            "  Run 2: 2ce95e6396859fc0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 378.57ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempt 1 failed: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:400 POST /v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 353.62ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempt 2 failed: 400 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-3.0-pro-preview:generateContent?%24alt=json%3Benum-encoding%3Dint: API key not valid. Please pass a valid API key.\n",
            "  Falling back to fuzzy matching\n",
            "  Run 3: 25af872f9906953b...\n",
            "  âœ— FAIL: 3 different outputs\n",
            "\n",
            "[UNICODE CHECK]\n",
            "  âœ“ PASS: Found diacritics: {'È›', 'ÃŽ', 'Ã®', 'Äƒ', 'È˜', 'È™', 'Ä‚', 'Ã¢'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bee85f18-270c-448d-bc78-ac4389451b28\", \"sample_forms.filled.docx\", 50748)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Download started!\n",
            "\n",
            "ðŸŽ‰ COMPLETE!\n"
          ]
        }
      ]
    }
  ]
}